{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Array([[[0.4840707 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.02204512, 0.        ],\n",
      "        [0.0693898 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.02279956, 0.        ],\n",
      "        [0.344575  , 0.        , 0.        , ..., 0.        ,\n",
      "         0.01206323, 0.        ],\n",
      "        ...,\n",
      "        [0.6926438 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.1365541 , 0.00513582],\n",
      "        [0.63773996, 0.        , 0.        , ..., 0.        ,\n",
      "         0.04622685, 0.        ],\n",
      "        [0.85276425, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32), Array([[[0.7025213 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.07945526, 0.        ],\n",
      "        [0.8328907 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.18241775, 0.        ],\n",
      "        [0.45594347, 0.        , 0.        , ..., 0.        ,\n",
      "         0.02872106, 0.        ],\n",
      "        ...,\n",
      "        [0.01685721, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.44803602, 0.        , 0.        , ..., 0.        ,\n",
      "         0.014068  , 0.        ],\n",
      "        [0.5338059 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32), Array([[[0.28406078, 0.        , 0.        , ..., 0.        ,\n",
      "         0.06348436, 0.        ],\n",
      "        [0.7490519 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.29900512, 0.01198368],\n",
      "        [0.3263911 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.17672208, 0.        ],\n",
      "        ...,\n",
      "        [0.56971633, 0.        , 0.        , ..., 0.        ,\n",
      "         0.08252369, 0.        ],\n",
      "        [0.28114402, 0.        , 0.        , ..., 0.        ,\n",
      "         0.02736305, 0.        ],\n",
      "        [0.8158474 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32), Array([[[0.4562771 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.03869452, 0.        ],\n",
      "        [0.7160025 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.04898646, 0.        ],\n",
      "        [0.8674495 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.10941476, 0.        ],\n",
      "        ...,\n",
      "        [0.33909512, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.6618413 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.04841322, 0.        ],\n",
      "        [0.3060679 , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import chex\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "_Array = chex.Array\n",
    "_Fn = Callable[..., Any]\n",
    "BIG_NUMBER = 1e6\n",
    "PROCESSOR_TAG = 'clrs_processor'\n",
    "\n",
    "\n",
    "class Processor(hk.Module):\n",
    "  \"\"\"Processor abstract base class.\"\"\"\n",
    "\n",
    "  def __init__(self, name: str):\n",
    "    if not name.endswith(PROCESSOR_TAG):\n",
    "      name = name + '_' + PROCESSOR_TAG\n",
    "    super().__init__(name=name)\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def __call__(\n",
    "      self,\n",
    "      node_fts: _Array,\n",
    "      edge_fts: _Array,\n",
    "      graph_fts: _Array,\n",
    "      adj_mat: _Array,\n",
    "      hidden: _Array,\n",
    "      **kwargs,\n",
    "  ) -> Tuple[_Array, Optional[_Array]]:\n",
    "    \"\"\"Processor inference step.\n",
    "\n",
    "    Args:\n",
    "      node_fts: Node features.\n",
    "      edge_fts: Edge features.\n",
    "      graph_fts: Graph features.\n",
    "      adj_mat: Graph adjacency matrix.\n",
    "      hidden: Hidden features.\n",
    "      **kwargs: Extra kwargs.\n",
    "\n",
    "    Returns:\n",
    "      Output of processor inference step as a 2-tuple of (node, edge)\n",
    "      embeddings. The edge embeddings can be None.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @property\n",
    "  def inf_bias(self):\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def inf_bias_edge(self):\n",
    "    return False\n",
    "\n",
    "\n",
    "class GAT(Processor):\n",
    "  \"\"\"Graph Attention Network (Velickovic et al., ICLR 2018).\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      out_size: int,\n",
    "      nb_heads: int,\n",
    "      activation: Optional[_Fn] = jax.nn.relu,\n",
    "      residual: bool = True,\n",
    "      use_ln: bool = False,\n",
    "      name: str = 'gat_aggr',\n",
    "  ):\n",
    "    super().__init__(name=name)\n",
    "    self.out_size = out_size\n",
    "    self.nb_heads = nb_heads\n",
    "    if out_size % nb_heads != 0:\n",
    "      raise ValueError('The number of attention heads must divide the width!')\n",
    "    self.head_size = out_size // nb_heads\n",
    "    self.activation = activation\n",
    "    self.residual = residual\n",
    "    self.use_ln = use_ln\n",
    "\n",
    "  def __call__(  # pytype: disable=signature-mismatch  # numpy-scalars\n",
    "      self,\n",
    "      node_fts: _Array,\n",
    "      edge_fts: _Array,\n",
    "      graph_fts: _Array,\n",
    "      adj_mat: _Array,\n",
    "      hidden: _Array,\n",
    "      **unused_kwargs,\n",
    "  ) -> _Array:\n",
    "    \"\"\"GAT inference step.\"\"\"\n",
    "\n",
    "    b, n, _ = node_fts.shape\n",
    "    assert edge_fts.shape[:-1] == (b, n, n)\n",
    "    assert graph_fts.shape[:-1] == (b,)\n",
    "    assert adj_mat.shape == (b, n, n)\n",
    "\n",
    "    z = jnp.concatenate([node_fts, hidden], axis=-1)\n",
    "    m = hk.Linear(self.out_size)\n",
    "    skip = hk.Linear(self.out_size)\n",
    "\n",
    "    bias_mat = (adj_mat - 1.0) * 1e9\n",
    "    bias_mat = jnp.tile(bias_mat[..., None],\n",
    "                        (1, 1, 1, self.nb_heads))     # [B, N, N, H]\n",
    "    bias_mat = jnp.transpose(bias_mat, (0, 3, 1, 2))  # [B, H, N, N]\n",
    "\n",
    "    a_1 = hk.Linear(self.nb_heads)\n",
    "    a_2 = hk.Linear(self.nb_heads)\n",
    "    a_e = hk.Linear(self.nb_heads)\n",
    "    a_g = hk.Linear(self.nb_heads)\n",
    "\n",
    "    values = m(z)                                      # [B, N, H*F]\n",
    "    values = jnp.reshape(\n",
    "        values,\n",
    "        values.shape[:-1] + (self.nb_heads, self.head_size))  # [B, N, H, F]\n",
    "    values = jnp.transpose(values, (0, 2, 1, 3))              # [B, H, N, F]\n",
    "\n",
    "    att_1 = jnp.expand_dims(a_1(z), axis=-1)\n",
    "    att_2 = jnp.expand_dims(a_2(z), axis=-1)\n",
    "    att_e = a_e(edge_fts)\n",
    "    att_g = jnp.expand_dims(a_g(graph_fts), axis=-1)\n",
    "\n",
    "    logits = (\n",
    "        jnp.transpose(att_1, (0, 2, 1, 3)) +  # + [B, H, N, 1]\n",
    "        jnp.transpose(att_2, (0, 2, 3, 1)) +  # + [B, H, 1, N]\n",
    "        jnp.transpose(att_e, (0, 3, 1, 2)) +  # + [B, H, N, N]\n",
    "        jnp.expand_dims(att_g, axis=-1)       # + [B, H, 1, 1]\n",
    "    )                                         # = [B, H, N, N]\n",
    "    coefs = jax.nn.softmax(jax.nn.leaky_relu(logits) + bias_mat, axis=-1)\n",
    "    ret = jnp.matmul(coefs, values)  # [B, H, N, F]\n",
    "    ret = jnp.transpose(ret, (0, 2, 1, 3))  # [B, N, H, F]\n",
    "    ret = jnp.reshape(ret, ret.shape[:-2] + (self.out_size,))  # [B, N, H*F]\n",
    "\n",
    "    if self.residual:\n",
    "      ret += skip(z)\n",
    "\n",
    "    if self.activation is not None:\n",
    "      ret = self.activation(ret)\n",
    "\n",
    "    if self.use_ln:\n",
    "      ln = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "      ret = ln(ret)\n",
    "\n",
    "    return ret, None  # pytype: disable=bad-return-type  # numpy-scalars\n",
    "\n",
    "\n",
    "class GATFull(GAT):\n",
    "  \"\"\"Graph Attention Network with full adjacency matrix.\"\"\"\n",
    "\n",
    "  def __call__(self, node_fts: _Array, edge_fts: _Array, graph_fts: _Array,\n",
    "               adj_mat: _Array, hidden: _Array, **unused_kwargs) -> _Array:\n",
    "    adj_mat = jnp.ones_like(adj_mat)\n",
    "    return super().__call__(node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "\n",
    "\n",
    "class GATv2(Processor):\n",
    "  \"\"\"Graph Attention Network v2 (Brody et al., ICLR 2022).\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      out_size: int,\n",
    "      nb_heads: int,\n",
    "      mid_size: Optional[int] = None,\n",
    "      activation: Optional[_Fn] = jax.nn.relu,\n",
    "      residual: bool = True,\n",
    "      use_ln: bool = False,\n",
    "      name: str = 'gatv2_aggr',\n",
    "  ):\n",
    "    super().__init__(name=name)\n",
    "    if mid_size is None:\n",
    "      self.mid_size = out_size\n",
    "    else:\n",
    "      self.mid_size = mid_size\n",
    "    self.out_size = out_size\n",
    "    self.nb_heads = nb_heads\n",
    "    if out_size % nb_heads != 0:\n",
    "      raise ValueError('The number of attention heads must divide the width!')\n",
    "    self.head_size = out_size // nb_heads\n",
    "    if self.mid_size % nb_heads != 0:\n",
    "      raise ValueError('The number of attention heads must divide the message!')\n",
    "    self.mid_head_size = self.mid_size // nb_heads\n",
    "    self.activation = activation\n",
    "    self.residual = residual\n",
    "    self.use_ln = use_ln\n",
    "\n",
    "  def __call__(  # pytype: disable=signature-mismatch  # numpy-scalars\n",
    "      self,\n",
    "      node_fts: _Array,\n",
    "      edge_fts: _Array,\n",
    "      graph_fts: _Array,\n",
    "      adj_mat: _Array,\n",
    "      hidden: _Array,\n",
    "      **unused_kwargs,\n",
    "  ) -> _Array:\n",
    "    \"\"\"GATv2 inference step.\"\"\"\n",
    "\n",
    "    b, n, _ = node_fts.shape\n",
    "    assert edge_fts.shape[:-1] == (b, n, n)\n",
    "    assert graph_fts.shape[:-1] == (b,)\n",
    "    assert adj_mat.shape == (b, n, n)\n",
    "\n",
    "    z = jnp.concatenate([node_fts, hidden], axis=-1)\n",
    "    m = hk.Linear(self.out_size)\n",
    "    skip = hk.Linear(self.out_size)\n",
    "\n",
    "    bias_mat = (adj_mat - 1.0) * 1e9\n",
    "    bias_mat = jnp.tile(bias_mat[..., None],\n",
    "                        (1, 1, 1, self.nb_heads))     # [B, N, N, H]\n",
    "    bias_mat = jnp.transpose(bias_mat, (0, 3, 1, 2))  # [B, H, N, N]\n",
    "\n",
    "    w_1 = hk.Linear(self.mid_size)\n",
    "    w_2 = hk.Linear(self.mid_size)\n",
    "    w_e = hk.Linear(self.mid_size)\n",
    "    w_g = hk.Linear(self.mid_size)\n",
    "\n",
    "    a_heads = []\n",
    "    for _ in range(self.nb_heads):\n",
    "      a_heads.append(hk.Linear(1))\n",
    "\n",
    "    values = m(z)                                      # [B, N, H*F]\n",
    "    values = jnp.reshape(\n",
    "        values,\n",
    "        values.shape[:-1] + (self.nb_heads, self.head_size))  # [B, N, H, F]\n",
    "    values = jnp.transpose(values, (0, 2, 1, 3))              # [B, H, N, F]\n",
    "\n",
    "    pre_att_1 = w_1(z)\n",
    "    pre_att_2 = w_2(z)\n",
    "    pre_att_e = w_e(edge_fts)\n",
    "    pre_att_g = w_g(graph_fts)\n",
    "\n",
    "    pre_att = (\n",
    "        jnp.expand_dims(pre_att_1, axis=1) +     # + [B, 1, N, H*F]\n",
    "        jnp.expand_dims(pre_att_2, axis=2) +     # + [B, N, 1, H*F]\n",
    "        pre_att_e +                              # + [B, N, N, H*F]\n",
    "        jnp.expand_dims(pre_att_g, axis=(1, 2))  # + [B, 1, 1, H*F]\n",
    "    )                                            # = [B, N, N, H*F]\n",
    "\n",
    "    pre_att = jnp.reshape(\n",
    "        pre_att,\n",
    "        pre_att.shape[:-1] + (self.nb_heads, self.mid_head_size)\n",
    "    )  # [B, N, N, H, F]\n",
    "\n",
    "    pre_att = jnp.transpose(pre_att, (0, 3, 1, 2, 4))  # [B, H, N, N, F]\n",
    "\n",
    "    # This part is not very efficient, but we agree to keep it this way to\n",
    "    # enhance readability, assuming `nb_heads` will not be large.\n",
    "    logit_heads = []\n",
    "    for head in range(self.nb_heads):\n",
    "      logit_heads.append(\n",
    "          jnp.squeeze(\n",
    "              a_heads[head](jax.nn.leaky_relu(pre_att[:, head])),\n",
    "              axis=-1)\n",
    "      )  # [B, N, N]\n",
    "\n",
    "    logits = jnp.stack(logit_heads, axis=1)  # [B, H, N, N]\n",
    "\n",
    "    coefs = jax.nn.softmax(logits + bias_mat, axis=-1)\n",
    "    ret = jnp.matmul(coefs, values)  # [B, H, N, F]\n",
    "    ret = jnp.transpose(ret, (0, 2, 1, 3))  # [B, N, H, F]\n",
    "    ret = jnp.reshape(ret, ret.shape[:-2] + (self.out_size,))  # [B, N, H*F]\n",
    "\n",
    "    if self.residual:\n",
    "      ret += skip(z)\n",
    "\n",
    "    if self.activation is not None:\n",
    "      ret = self.activation(ret)\n",
    "\n",
    "    if self.use_ln:\n",
    "      ln = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "      ret = ln(ret)\n",
    "\n",
    "    return ret, None  # pytype: disable=bad-return-type  # numpy-scalars\n",
    "\n",
    "\n",
    "class GATv2FullD2(GATv2):\n",
    "  \"\"\"Graph Attention Network v2 with full adjacency matrix and D2 symmetry.\"\"\"\n",
    "\n",
    "  def d2_forward(self,\n",
    "                 node_fts: List[_Array],\n",
    "                 edge_fts: List[_Array],\n",
    "                 graph_fts: List[_Array],\n",
    "                 adj_mat: _Array,\n",
    "                 hidden: _Array,\n",
    "                 **unused_kwargs) -> List[_Array]:\n",
    "    num_d2_actions = 4\n",
    "\n",
    "    d2_inverses = [\n",
    "        0, 1, 2, 3  # All members of D_2 are self-inverses!\n",
    "    ]\n",
    "\n",
    "    d2_multiply = [\n",
    "        [0, 1, 2, 3],\n",
    "        [1, 0, 3, 2],\n",
    "        [2, 3, 0, 1],\n",
    "        [3, 2, 1, 0],\n",
    "    ]\n",
    "\n",
    "    assert len(node_fts) == num_d2_actions\n",
    "    assert len(edge_fts) == num_d2_actions\n",
    "    assert len(graph_fts) == num_d2_actions\n",
    "\n",
    "    ret_nodes = []\n",
    "    adj_mat = jnp.ones_like(adj_mat)\n",
    "\n",
    "    for g in range(num_d2_actions):\n",
    "      emb_values = []\n",
    "      for h in range(num_d2_actions):\n",
    "        gh = d2_multiply[d2_inverses[g]][h]\n",
    "        node_features = jnp.concatenate(\n",
    "            (node_fts[g], node_fts[gh]),\n",
    "            axis=-1)\n",
    "        edge_features = jnp.concatenate(\n",
    "            (edge_fts[g], edge_fts[gh]),\n",
    "            axis=-1)\n",
    "        graph_features = jnp.concatenate(\n",
    "            (graph_fts[g], graph_fts[gh]),\n",
    "            axis=-1)\n",
    "        cell_embedding = super().__call__(\n",
    "            node_fts=node_features,\n",
    "            edge_fts=edge_features,\n",
    "            graph_fts=graph_features,\n",
    "            adj_mat=adj_mat,\n",
    "            hidden=hidden\n",
    "        )\n",
    "        emb_values.append(cell_embedding[0])\n",
    "      ret_nodes.append(\n",
    "          jnp.mean(jnp.stack(emb_values, axis=0), axis=0)\n",
    "      )\n",
    "\n",
    "    return ret_nodes\n",
    "\n",
    "\n",
    "class GATv2Full(GATv2):\n",
    "  \"\"\"Graph Attention Network v2 with full adjacency matrix.\"\"\"\n",
    "\n",
    "  def __call__(self, node_fts: _Array, edge_fts: _Array, graph_fts: _Array,\n",
    "               adj_mat: _Array, hidden: _Array, **unused_kwargs) -> _Array:\n",
    "    adj_mat = jnp.ones_like(adj_mat)\n",
    "    return super().__call__(node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "\n",
    "# main\n",
    "\n",
    "# Assuming you want a batch size of 1 for simplicity\n",
    "batch_size = 1\n",
    "\n",
    "# Number of players, features, and latent features\n",
    "n_players = 10\n",
    "n_node_features = 5\n",
    "n_edge_features = 3\n",
    "n_graph_features = 2\n",
    "n_latent_features = 4\n",
    "\n",
    "# Add a batch dimension to your data\n",
    "node_fts = [jnp.expand_dims(jnp.array(np.random.rand(n_players, n_node_features)), axis=0) for _ in range(4)]\n",
    "edge_fts = [jnp.expand_dims(jnp.array(np.random.rand(n_players, n_players, n_edge_features)), axis=0) for _ in range(4)]\n",
    "graph_fts = [jnp.expand_dims(jnp.array(np.random.rand(n_graph_features)), axis=0) for _ in range(4)]\n",
    "adj_mat = jnp.expand_dims(jnp.array(np.random.randint(0, 2, (n_players, n_players))), axis=0)\n",
    "hidden = jnp.expand_dims(jnp.zeros((n_players, n_latent_features)), axis=0)\n",
    "\n",
    "\n",
    "def forward(node_fts, edge_fts, graph_fts, adj_mat, hidden):\n",
    "    # Instantiate your GATv2FullD2 here, within the function.\n",
    "    out_size = 128  \n",
    "    nb_heads = 8\n",
    "    model = GATv2FullD2(out_size=out_size, nb_heads=nb_heads)\n",
    "    return model.d2_forward(node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "\n",
    "# Transform the function using Haiku\n",
    "transformed_forward = hk.transform(forward)\n",
    "\n",
    "# Create a JAX random key for initialization\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize the model (parameters)\n",
    "params = transformed_forward.init(rng, node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "\n",
    "# Apply the model\n",
    "output = transformed_forward.apply(params, rng, node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 13:18:20.381933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/rgildin/miniconda3/envs/py310_tacticai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import clrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler, spec = clrs.build_sampler(\n",
    "    name='bfs',\n",
    "    seed=42,\n",
    "    num_samples=1000,\n",
    "    length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clrs.examples.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.hint_mode == 'encoded_decoded':\n",
    "    encode_hints = True\n",
    "    decode_hints = True\n",
    "elif FLAGS.hint_mode == 'decoded_only':\n",
    "    encode_hints = False\n",
    "    decode_hints = True\n",
    "elif FLAGS.hint_mode == 'none':\n",
    "    encode_hints = False\n",
    "    decode_hints = False\n",
    "else:\n",
    "    raise ValueError('Hint mode not in {encoded_decoded, decoded_only, none}.')\n",
    "\n",
    "train_lengths = [int(x) for x in FLAGS.train_lengths]\n",
    "\n",
    "rng = np.random.RandomState(FLAGS.seed)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2**32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samplers\n",
    "(train_samplers,\n",
    "val_samplers, val_sample_counts,\n",
    "test_samplers, test_sample_counts,\n",
    "spec_list) = create_samplers(rng, train_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_samplers(rng, train_lengths: List[int]):\n",
    "\"\"\"Create all the samplers.\"\"\"\n",
    "train_samplers = []\n",
    "val_samplers = []\n",
    "val_sample_counts = []\n",
    "test_samplers = []\n",
    "test_sample_counts = []\n",
    "spec_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/tmp/CLRS30/CLRS30_v1.0.0/clrs_dataset/activity_selector_test/1.0.0/clrs_dataset-test.tfrecord-00000-of-00001'\n",
    "data = tf.io.read_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "folder = '/tmp/CLRS30/CLRS30_v1.0.0/clrs_dataset/activity_selector_test'\n",
    "dataset = tfds.load(folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_tacticai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
