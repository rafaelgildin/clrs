{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import chex\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "_Array = chex.Array\n",
    "_Fn = Callable[..., Any]\n",
    "BIG_NUMBER = 1e6\n",
    "PROCESSOR_TAG = 'clrs_processor'\n",
    "\n",
    "\n",
    "class Processor(hk.Module):\n",
    "  \"\"\"Processor abstract base class.\"\"\"\n",
    "\n",
    "  def __init__(self, name: str):\n",
    "    if not name.endswith(PROCESSOR_TAG):\n",
    "      name = name + '_' + PROCESSOR_TAG\n",
    "    super().__init__(name=name)\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def __call__(\n",
    "      self,\n",
    "      node_fts: _Array,\n",
    "      edge_fts: _Array,\n",
    "      graph_fts: _Array,\n",
    "      adj_mat: _Array,\n",
    "      hidden: _Array,\n",
    "      **kwargs,\n",
    "  ) -> Tuple[_Array, Optional[_Array]]:\n",
    "    \"\"\"Processor inference step.\n",
    "\n",
    "    Args:\n",
    "      node_fts: Node features.\n",
    "      edge_fts: Edge features.\n",
    "      graph_fts: Graph features.\n",
    "      adj_mat: Graph adjacency matrix.\n",
    "      hidden: Hidden features.\n",
    "      **kwargs: Extra kwargs.\n",
    "\n",
    "    Returns:\n",
    "      Output of processor inference step as a 2-tuple of (node, edge)\n",
    "      embeddings. The edge embeddings can be None.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @property\n",
    "  def inf_bias(self):\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def inf_bias_edge(self):\n",
    "    return False\n",
    "\n",
    "\n",
    "class GAT(Processor):\n",
    "  \"\"\"Graph Attention Network (Velickovic et al., ICLR 2018).\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      out_size: int,\n",
    "      nb_heads: int,\n",
    "      activation: Optional[_Fn] = jax.nn.relu,\n",
    "      residual: bool = True,\n",
    "      use_ln: bool = False,\n",
    "      name: str = 'gat_aggr',\n",
    "  ):\n",
    "    super().__init__(name=name)\n",
    "    self.out_size = out_size\n",
    "    self.nb_heads = nb_heads\n",
    "    if out_size % nb_heads != 0:\n",
    "      raise ValueError('The number of attention heads must divide the width!')\n",
    "    self.head_size = out_size // nb_heads\n",
    "    self.activation = activation\n",
    "    self.residual = residual\n",
    "    self.use_ln = use_ln\n",
    "\n",
    "  def __call__(  # pytype: disable=signature-mismatch  # numpy-scalars\n",
    "      self,\n",
    "      node_fts: _Array,\n",
    "      edge_fts: _Array,\n",
    "      graph_fts: _Array,\n",
    "      adj_mat: _Array,\n",
    "      hidden: _Array,\n",
    "      **unused_kwargs,\n",
    "  ) -> _Array:\n",
    "    \"\"\"GAT inference step.\"\"\"\n",
    "\n",
    "    b, n, _ = node_fts.shape\n",
    "    assert edge_fts.shape[:-1] == (b, n, n)\n",
    "    assert graph_fts.shape[:-1] == (b,)\n",
    "    assert adj_mat.shape == (b, n, n)\n",
    "\n",
    "    z = jnp.concatenate([node_fts, hidden], axis=-1)\n",
    "    m = hk.Linear(self.out_size)\n",
    "    skip = hk.Linear(self.out_size)\n",
    "\n",
    "    bias_mat = (adj_mat - 1.0) * 1e9\n",
    "    bias_mat = jnp.tile(bias_mat[..., None],\n",
    "                        (1, 1, 1, self.nb_heads))     # [B, N, N, H]\n",
    "    bias_mat = jnp.transpose(bias_mat, (0, 3, 1, 2))  # [B, H, N, N]\n",
    "\n",
    "    a_1 = hk.Linear(self.nb_heads)\n",
    "    a_2 = hk.Linear(self.nb_heads)\n",
    "    a_e = hk.Linear(self.nb_heads)\n",
    "    a_g = hk.Linear(self.nb_heads)\n",
    "\n",
    "    values = m(z)                                      # [B, N, H*F]\n",
    "    values = jnp.reshape(\n",
    "        values,\n",
    "        values.shape[:-1] + (self.nb_heads, self.head_size))  # [B, N, H, F]\n",
    "    values = jnp.transpose(values, (0, 2, 1, 3))              # [B, H, N, F]\n",
    "\n",
    "    att_1 = jnp.expand_dims(a_1(z), axis=-1)\n",
    "    att_2 = jnp.expand_dims(a_2(z), axis=-1)\n",
    "    att_e = a_e(edge_fts)\n",
    "    att_g = jnp.expand_dims(a_g(graph_fts), axis=-1)\n",
    "\n",
    "    logits = (\n",
    "        jnp.transpose(att_1, (0, 2, 1, 3)) +  # + [B, H, N, 1]\n",
    "        jnp.transpose(att_2, (0, 2, 3, 1)) +  # + [B, H, 1, N]\n",
    "        jnp.transpose(att_e, (0, 3, 1, 2)) +  # + [B, H, N, N]\n",
    "        jnp.expand_dims(att_g, axis=-1)       # + [B, H, 1, 1]\n",
    "    )                                         # = [B, H, N, N]\n",
    "    coefs = jax.nn.softmax(jax.nn.leaky_relu(logits) + bias_mat, axis=-1)\n",
    "    ret = jnp.matmul(coefs, values)  # [B, H, N, F]\n",
    "    ret = jnp.transpose(ret, (0, 2, 1, 3))  # [B, N, H, F]\n",
    "    ret = jnp.reshape(ret, ret.shape[:-2] + (self.out_size,))  # [B, N, H*F]\n",
    "\n",
    "    if self.residual:\n",
    "      ret += skip(z)\n",
    "\n",
    "    if self.activation is not None:\n",
    "      ret = self.activation(ret)\n",
    "\n",
    "    if self.use_ln:\n",
    "      ln = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "      ret = ln(ret)\n",
    "\n",
    "    return ret, None  # pytype: disable=bad-return-type  # numpy-scalars\n",
    "\n",
    "\n",
    "class GATFull(GAT):\n",
    "  \"\"\"Graph Attention Network with full adjacency matrix.\"\"\"\n",
    "\n",
    "  def __call__(self, node_fts: _Array, edge_fts: _Array, graph_fts: _Array,\n",
    "               adj_mat: _Array, hidden: _Array, **unused_kwargs) -> _Array:\n",
    "    adj_mat = jnp.ones_like(adj_mat)\n",
    "    return super().__call__(node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "\n",
    "\n",
    "class GATv2(Processor):\n",
    "  \"\"\"Graph Attention Network v2 (Brody et al., ICLR 2022).\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      out_size: int,\n",
    "      nb_heads: int,\n",
    "      mid_size: Optional[int] = None,\n",
    "      activation: Optional[_Fn] = jax.nn.relu,\n",
    "      residual: bool = True,\n",
    "      use_ln: bool = False,\n",
    "      name: str = 'gatv2_aggr',\n",
    "  ):\n",
    "    super().__init__(name=name)\n",
    "    if mid_size is None:\n",
    "      self.mid_size = out_size\n",
    "    else:\n",
    "      self.mid_size = mid_size\n",
    "    self.out_size = out_size\n",
    "    self.nb_heads = nb_heads\n",
    "    if out_size % nb_heads != 0:\n",
    "      raise ValueError('The number of attention heads must divide the width!')\n",
    "    self.head_size = out_size // nb_heads\n",
    "    if self.mid_size % nb_heads != 0:\n",
    "      raise ValueError('The number of attention heads must divide the message!')\n",
    "    self.mid_head_size = self.mid_size // nb_heads\n",
    "    self.activation = activation\n",
    "    self.residual = residual\n",
    "    self.use_ln = use_ln\n",
    "\n",
    "  def __call__(  # pytype: disable=signature-mismatch  # numpy-scalars\n",
    "      self,\n",
    "      node_fts: _Array,\n",
    "      edge_fts: _Array,\n",
    "      graph_fts: _Array,\n",
    "      adj_mat: _Array,\n",
    "      hidden: _Array,\n",
    "      **unused_kwargs,\n",
    "  ) -> _Array:\n",
    "    \"\"\"GATv2 inference step.\"\"\"\n",
    "\n",
    "    b, n, _ = node_fts.shape\n",
    "    assert edge_fts.shape[:-1] == (b, n, n)\n",
    "    assert graph_fts.shape[:-1] == (b,)\n",
    "    assert adj_mat.shape == (b, n, n)\n",
    "\n",
    "    z = jnp.concatenate([node_fts, hidden], axis=-1)\n",
    "    m = hk.Linear(self.out_size)\n",
    "    skip = hk.Linear(self.out_size)\n",
    "\n",
    "    bias_mat = (adj_mat - 1.0) * 1e9\n",
    "    bias_mat = jnp.tile(bias_mat[..., None],\n",
    "                        (1, 1, 1, self.nb_heads))     # [B, N, N, H]\n",
    "    bias_mat = jnp.transpose(bias_mat, (0, 3, 1, 2))  # [B, H, N, N]\n",
    "\n",
    "    w_1 = hk.Linear(self.mid_size)\n",
    "    w_2 = hk.Linear(self.mid_size)\n",
    "    w_e = hk.Linear(self.mid_size)\n",
    "    w_g = hk.Linear(self.mid_size)\n",
    "\n",
    "    a_heads = []\n",
    "    for _ in range(self.nb_heads):\n",
    "      a_heads.append(hk.Linear(1))\n",
    "\n",
    "    values = m(z)                                      # [B, N, H*F]\n",
    "    values = jnp.reshape(\n",
    "        values,\n",
    "        values.shape[:-1] + (self.nb_heads, self.head_size))  # [B, N, H, F]\n",
    "    values = jnp.transpose(values, (0, 2, 1, 3))              # [B, H, N, F]\n",
    "\n",
    "    pre_att_1 = w_1(z)\n",
    "    pre_att_2 = w_2(z)\n",
    "    pre_att_e = w_e(edge_fts)\n",
    "    pre_att_g = w_g(graph_fts)\n",
    "\n",
    "    pre_att = (\n",
    "        jnp.expand_dims(pre_att_1, axis=1) +     # + [B, 1, N, H*F]\n",
    "        jnp.expand_dims(pre_att_2, axis=2) +     # + [B, N, 1, H*F]\n",
    "        pre_att_e +                              # + [B, N, N, H*F]\n",
    "        jnp.expand_dims(pre_att_g, axis=(1, 2))  # + [B, 1, 1, H*F]\n",
    "    )                                            # = [B, N, N, H*F]\n",
    "\n",
    "    pre_att = jnp.reshape(\n",
    "        pre_att,\n",
    "        pre_att.shape[:-1] + (self.nb_heads, self.mid_head_size)\n",
    "    )  # [B, N, N, H, F]\n",
    "\n",
    "    pre_att = jnp.transpose(pre_att, (0, 3, 1, 2, 4))  # [B, H, N, N, F]\n",
    "\n",
    "    # This part is not very efficient, but we agree to keep it this way to\n",
    "    # enhance readability, assuming `nb_heads` will not be large.\n",
    "    logit_heads = []\n",
    "    for head in range(self.nb_heads):\n",
    "      logit_heads.append(\n",
    "          jnp.squeeze(\n",
    "              a_heads[head](jax.nn.leaky_relu(pre_att[:, head])),\n",
    "              axis=-1)\n",
    "      )  # [B, N, N]\n",
    "\n",
    "    logits = jnp.stack(logit_heads, axis=1)  # [B, H, N, N]\n",
    "\n",
    "    coefs = jax.nn.softmax(logits + bias_mat, axis=-1)\n",
    "    ret = jnp.matmul(coefs, values)  # [B, H, N, F]\n",
    "    ret = jnp.transpose(ret, (0, 2, 1, 3))  # [B, N, H, F]\n",
    "    ret = jnp.reshape(ret, ret.shape[:-2] + (self.out_size,))  # [B, N, H*F]\n",
    "\n",
    "    if self.residual:\n",
    "      ret += skip(z)\n",
    "\n",
    "    if self.activation is not None:\n",
    "      ret = self.activation(ret)\n",
    "\n",
    "    if self.use_ln:\n",
    "      ln = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "      ret = ln(ret)\n",
    "\n",
    "    return ret, None  # pytype: disable=bad-return-type  # numpy-scalars\n",
    "\n",
    "\n",
    "class GATv2FullD2(GATv2):\n",
    "  \"\"\"Graph Attention Network v2 with full adjacency matrix and D2 symmetry.\"\"\"\n",
    "\n",
    "  def d2_forward(self,\n",
    "                 node_fts: List[_Array],\n",
    "                 edge_fts: List[_Array],\n",
    "                 graph_fts: List[_Array],\n",
    "                 adj_mat: _Array,\n",
    "                 hidden: _Array,\n",
    "                 **unused_kwargs) -> List[_Array]:\n",
    "    num_d2_actions = 4\n",
    "\n",
    "    d2_inverses = [\n",
    "        0, 1, 2, 3  # All members of D_2 are self-inverses!\n",
    "    ]\n",
    "\n",
    "    d2_multiply = [\n",
    "        [0, 1, 2, 3],\n",
    "        [1, 0, 3, 2],\n",
    "        [2, 3, 0, 1],\n",
    "        [3, 2, 1, 0],\n",
    "    ]\n",
    "\n",
    "    assert len(node_fts) == num_d2_actions\n",
    "    assert len(edge_fts) == num_d2_actions\n",
    "    assert len(graph_fts) == num_d2_actions\n",
    "\n",
    "    ret_nodes = []\n",
    "    adj_mat = jnp.ones_like(adj_mat)\n",
    "\n",
    "    for g in range(num_d2_actions):\n",
    "      emb_values = []\n",
    "      for h in range(num_d2_actions):\n",
    "        gh = d2_multiply[d2_inverses[g]][h]\n",
    "        node_features = jnp.concatenate(\n",
    "            (node_fts[g], node_fts[gh]),\n",
    "            axis=-1)\n",
    "        edge_features = jnp.concatenate(\n",
    "            (edge_fts[g], edge_fts[gh]),\n",
    "            axis=-1)\n",
    "        graph_features = jnp.concatenate(\n",
    "            (graph_fts[g], graph_fts[gh]),\n",
    "            axis=-1)\n",
    "        cell_embedding = super().__call__(\n",
    "            node_fts=node_features,\n",
    "            edge_fts=edge_features,\n",
    "            graph_fts=graph_features,\n",
    "            adj_mat=adj_mat,\n",
    "            hidden=hidden\n",
    "        )\n",
    "        emb_values.append(cell_embedding[0])\n",
    "      ret_nodes.append(\n",
    "          jnp.mean(jnp.stack(emb_values, axis=0), axis=0)\n",
    "      )\n",
    "\n",
    "    return ret_nodes\n",
    "\n",
    "class GATv2Full(GATv2):\n",
    "  \"\"\"Graph Attention Network v2 with full adjacency matrix.\"\"\"\n",
    "\n",
    "  def __call__(self, node_fts: _Array, edge_fts: _Array, graph_fts: _Array,\n",
    "               adj_mat: _Array, hidden: _Array, **unused_kwargs) -> _Array:\n",
    "    adj_mat = jnp.ones_like(adj_mat)\n",
    "    return super().__call__(node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "\n",
    "def forward(node_fts, edge_fts, graph_fts, adj_mat, hidden):\n",
    "    # Instantiate your GATv2FullD2 here, within the function.\n",
    "    out_size = 128  \n",
    "    nb_heads = 8\n",
    "    model = GATv2FullD2(out_size=out_size, nb_heads=nb_heads)\n",
    "    return model.d2_forward(node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "\n",
    "# main\n",
    "batch_size = 1\n",
    "n_players = 22\n",
    "n_node_features = 5\n",
    "n_edge_features = 3\n",
    "n_graph_features = 2\n",
    "n_latent_features = 4\n",
    "\n",
    "# Create the data\n",
    "node_fts = [jnp.expand_dims(jnp.array(np.random.rand(n_players, n_node_features)), axis=0) for _ in range(4)]\n",
    "edge_fts = [jnp.expand_dims(jnp.array(np.random.rand(n_players, n_players, n_edge_features)), axis=0) for _ in range(4)]\n",
    "graph_fts = [jnp.expand_dims(jnp.array(np.random.rand(n_graph_features)), axis=0) for _ in range(4)]\n",
    "adj_mat = jnp.expand_dims(jnp.array(np.random.randint(0, 2, (n_players, n_players))), axis=0)\n",
    "hidden = jnp.expand_dims(jnp.zeros((n_players, n_latent_features)), axis=0)\n",
    "\n",
    "# Transform the function using Haiku\n",
    "transformed_forward = hk.transform(forward)\n",
    "\n",
    "# Create a JAX random key for initialization\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize the model (parameters)\n",
    "params = transformed_forward.init(rng, node_fts, edge_fts, graph_fts, adj_mat, hidden)\n",
    "\n",
    "# Apply the model\n",
    "output = transformed_forward.apply(params, rng, node_fts, edge_fts, graph_fts, adj_mat, hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04747611 0.04750526 0.0488289  0.04205194 0.04901259 0.0424196\n",
      "  0.04168977 0.04840901 0.04685603 0.04676333 0.04718176 0.04297247\n",
      "  0.04319186 0.04787761 0.04247737 0.04570602 0.04057295 0.04831294\n",
      "  0.04313481 0.04904582 0.04600187 0.04251195]]\n"
     ]
    }
   ],
   "source": [
    "# Define a linear layer to compute scores\n",
    "class ReceiverPredictor(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.linear = hk.Linear(1)\n",
    "    \n",
    "    def __call__(self, embeddings):\n",
    "        return self.linear(embeddings)\n",
    "\n",
    "# Initialize and apply the linear layer\n",
    "def receiver_prediction_forward(embeddings):\n",
    "    model = ReceiverPredictor()\n",
    "    return model(embeddings)\n",
    "\n",
    "# Average the embeddings across the four transformed views\n",
    "averaged_embeddings = jnp.mean(jnp.stack(output), axis=0)\n",
    "\n",
    "# Transform the receiver prediction function using Haiku\n",
    "transformed_receiver_prediction = hk.transform(receiver_prediction_forward)\n",
    "# Initialize the receiver prediction model (parameters)\n",
    "params_receiver = transformed_receiver_prediction.init(rng, averaged_embeddings)\n",
    "# Apply the receiver prediction model to get scores\n",
    "scores = transformed_receiver_prediction.apply(params_receiver, rng, averaged_embeddings)\n",
    "\n",
    "# Squeeze the scores to remove the last dimension and apply softmax\n",
    "scores = jnp.squeeze(scores, axis=-1)\n",
    "probabilities = jax.nn.softmax(scores, axis=-1)\n",
    "\n",
    "# Print the probabilities\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5949052]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Average the embeddings across the four transformed views\n",
    "averaged_embeddings = jnp.mean(jnp.stack(output), axis=0)\n",
    "\n",
    "# Step 2: Define a linear layer to compute the shot prediction score\n",
    "class ShotPredictor(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.linear = hk.Linear(1)\n",
    "    \n",
    "    def __call__(self, embeddings):\n",
    "        global_embedding = jnp.mean(embeddings, axis=1)\n",
    "        return self.linear(global_embedding)\n",
    "\n",
    "# Step 3: Initialize and apply the linear layer for shot prediction\n",
    "def shot_prediction_forward(embeddings):\n",
    "    model = ShotPredictor()\n",
    "    return model(embeddings)\n",
    "\n",
    "# Transform the shot prediction function using Haiku\n",
    "transformed_shot_prediction = hk.transform(shot_prediction_forward)\n",
    "\n",
    "# Create a JAX random key for initialization\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize the shot prediction model (parameters)\n",
    "params_shot = transformed_shot_prediction.init(rng, averaged_embeddings)\n",
    "\n",
    "# Apply the shot prediction model to get the score\n",
    "score = transformed_shot_prediction.apply(params_shot, rng, averaged_embeddings)\n",
    "\n",
    "# Step 4: Apply sigmoid to convert the score into a probability\n",
    "shot_probability = jax.nn.sigmoid(score)\n",
    "\n",
    "# Print the shot probability\n",
    "print(shot_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggest adjustments in player positions and velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Positions: [[[-0.15373099 -0.5776852 ]\n",
      "  [-0.54797816  0.349627  ]\n",
      "  [ 0.796337    0.27583018]\n",
      "  [ 0.8361689   0.658131  ]\n",
      "  [-1.1743636   0.26393038]\n",
      "  [ 1.1182784   0.09397618]\n",
      "  [-0.972666    0.57598615]\n",
      "  [ 0.88302577 -1.1866739 ]\n",
      "  [ 0.38232225 -0.19630621]\n",
      "  [-1.4119033  -0.42984524]\n",
      "  [-1.0729328   1.243825  ]\n",
      "  [-1.4856875   0.30136743]\n",
      "  [-0.2459184   1.2662115 ]\n",
      "  [-2.2002583   0.9062169 ]\n",
      "  [-2.3522437   0.55627424]\n",
      "  [ 0.46456093  0.10938837]\n",
      "  [ 1.0150944   0.8138926 ]\n",
      "  [-0.4559812   2.5382173 ]\n",
      "  [-0.50415194  0.20640746]\n",
      "  [-0.810416   -1.6976104 ]\n",
      "  [ 0.7919555   1.781926  ]\n",
      "  [ 1.92274     1.7019848 ]]]\n",
      "New Velocities: [[[ 0.29783598  0.17285213]\n",
      "  [ 0.4111511  -1.1176956 ]\n",
      "  [-0.5487272  -0.26066583]\n",
      "  [ 0.83609235 -0.8582128 ]\n",
      "  [ 0.7402787  -1.2575378 ]\n",
      "  [ 0.74992824  1.0300466 ]\n",
      "  [ 0.05962151  0.18771464]\n",
      "  [ 1.0322506  -1.770295  ]\n",
      "  [ 0.619172    0.41612193]\n",
      "  [ 0.2987381  -0.21577743]\n",
      "  [-0.22905567  0.49388447]\n",
      "  [ 0.2659631  -0.33611947]\n",
      "  [ 0.14251202  0.44241855]\n",
      "  [ 0.6024567  -0.35487482]\n",
      "  [ 0.12126867 -0.99268603]\n",
      "  [ 0.33811527 -0.07427019]\n",
      "  [ 0.43051213  0.4802556 ]\n",
      "  [ 0.18458572 -0.02306724]\n",
      "  [ 0.18936382  0.25749195]\n",
      "  [ 0.5929083  -0.05316409]\n",
      "  [ 0.22503403 -0.962477  ]\n",
      "  [ 0.2814343   0.40516797]]]\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "# Step 1: Average the embeddings across the four transformed views\n",
    "averaged_embeddings = jnp.mean(jnp.stack(output), axis=0)\n",
    "\n",
    "# Step 2: Define a VAE (Variational Autoencoder) to generate new player positions and velocities\n",
    "class TacticGenerator(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.encoder = hk.Linear(64)\n",
    "        self.mean = hk.Linear(4)  # For positions (x, y) and velocities (vx, vy)\n",
    "        self.log_std = hk.Linear(4)  # For positions (x, y) and velocities (vx, vy)\n",
    "\n",
    "    def __call__(self, embeddings, rng):\n",
    "        # Encoder\n",
    "        encoded = self.encoder(embeddings)\n",
    "        \n",
    "        # Generate latent variables for positions and velocities\n",
    "        mean = self.mean(encoded)\n",
    "        log_std = self.log_std(encoded)\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        eps = random.normal(rng, mean.shape)\n",
    "        z = mean + eps * std\n",
    "        \n",
    "        return z, mean, log_std\n",
    "\n",
    "# Step 3: Initialize and apply the VAE for tactic generation\n",
    "def tactic_generation_forward(embeddings, rng):\n",
    "    model = TacticGenerator()\n",
    "    return model(embeddings, rng)\n",
    "\n",
    "# Transform the tactic generation function using Haiku\n",
    "transformed_tactic_generation = hk.transform(tactic_generation_forward)\n",
    "\n",
    "# Create a JAX random key for initialization\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize the tactic generation model (parameters)\n",
    "params_tactic = transformed_tactic_generation.init(rng, averaged_embeddings, rng)\n",
    "\n",
    "# Apply the tactic generation model to generate new positions and velocities\n",
    "new_positions_velocities, mean, log_std = transformed_tactic_generation.apply(params_tactic, rng, averaged_embeddings, rng)\n",
    "\n",
    "# Extract positions and velocities from the generated output\n",
    "new_positions = new_positions_velocities[:, :, :2]  # First two dimensions for positions (x, y)\n",
    "new_velocities = new_positions_velocities[:, :, 2:]  # Last two dimensions for velocities (vx, vy)\n",
    "\n",
    "# Print the new positions and velocities\n",
    "print(\"New Positions:\", new_positions)\n",
    "print(\"New Velocities:\", new_velocities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_tacticai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
